---
title: "Assignment 2: Group 45"
author: "Daniel Engbert, Rik Timmer, Koen van der Pool"
date: "03 March 2023"
output: pdf_document
fontsize: 11pt
highlight: tango
editor_options: 
  markdown: 
    wrap: 72
---

```{r, echo=FALSE}
options(digits=3) # e.g. for anova and summary() outputs...

# helper function to check normality throughout this assignment
checkNorm = function(data, name, unit="") {
  xlab = name
  if (unit != "") {
    xlab = sprintf("%s (%s)", name, unit)
  }
  hist(data, main=sprintf("Histogram of %s", name), xlab=xlab)
  qqnorm(data, main=sprintf("Normal qqplot of %s", name))
  res = shapiro.test(data)
  print(sprintf("Shapiro-Wilk normality p-value for %s: %.3f", name, res$p.value))
  return(res$p.value)
}

printPval = function(pval) {
  print(sprintf("p-value = %.3f", pval))
}

conclude = function(pval, hypothesis, type="test") {
  if (pval < 0.05) {
    return(sprintf("the p-value $%.3f<0.05$ for this %s suggests is sufficient evidence to reject the $H_0$ (%s).", pval, type, hypothesis));
  } else {
    return(sprintf("the p-value $%.3f>0.05$ for this %s suggests there's insufficient evidence to reject the $H_0$ (%s).", pval, type, hypothesis));
  }
}
```

Note: we made a function `checkNorm()` which prints a histogram, qqplot,
and p-value from the shapiro-wilk normality test. And we made a function
`printPval()` which simply prints a given p-value to 3 significant
figures. We utilize both functions throughout this assignment.

## Exercise 1: Trees

### 1 a)

<!--# TODO: check assumptions for ANOVA regarding normality?? -->

```{r}
trees = read.table("treeVolume.txt", header=T)
model = lm(volume~type, data=trees)
print("model coefficients:"); summary(model)$coefficients
res = anova(model)
sprintf("ANOVA p-value for type = %.3f", res["type", "Pr(>F)"])
```

The p-value $0.174>0.05$ for the type in the ANOVA analysis of the
linear model, suggests there's insufficient evidence to reject the $H_0$
(that tree type influences volume).

```{r, fig.height=4.5, fig.width=4.5, echo=FALSE}
oaks = dplyr::filter(trees, type == "oak")
beeches = dplyr::filter(trees, type != "oak")
par(mfrow = c(2, 2))
# making plots 
pval = checkNorm(oaks$volume, "oak")
pval = checkNorm(beeches$volume, "beech")

sprintf("oak mean volume = %.3f, beech mean volume = %.3f", mean(oaks$volume), mean(beeches$volume))

# TODO: we don't have to actually do this t test
#res = t.test(oaks$volume, beeches$volume)
#printPval(res$p.value)
```

<!--# TODO: beech isn't normal, oak appears somewhat doubtful, so maybe we can say "it could be done in principle IF the data was normal, but that's not the case here..." -->

We can split the data into two samples of tree volume based on the tree
types, and compare the means of the samples using a t-test to determine
whether, based on this data, there is a significant difference in mean
volume between the two tree types. As can be seen in the output of the
t-test $0.166 > 0.05$, signifying once again that there is not enough
evidence to reject the null hypothesis that the means of the samples are
the same. This concurs with the results of the ANOVA.

```{r}
new_oak = data.frame(type="oak"); new_beech = data.frame(type = "beech")
pred1 = predict(model, new_oak); pred2 = predict(model, new_beech)
sprintf("predicted volumes: oak = %.3f, beech = %.3f", pred1, pred2)
```

### 1 b)

```{r}
model = lm(volume~type*diameter + height, data=trees)
res = anova(model)
sprintf("ANOVA p-value for type:diameter = %.3f", res["type:diameter", "Pr(>F)"])
```

We built a linear model that added an interaction term between diameter
and type,
`r conclude(res["type:diameter", "Pr(>F)"], "that the influence of diameter on volume is the same for both tree types", type="term")`

```{r}
model = lm(volume~type*height + diameter, data=trees)
res = anova(model)
sprintf("ANOVA p-value for type:diameter = %.3f", res["type:height", "Pr(>F)"])
```

Now running another linear model that includes an interaction term
between height and type instead,
`r conclude(res["type:height", "Pr(>F)"], "that the influence of height on volume is the same for both tree types", type="term")`

So based on the results from our two models above, there's insufficient
evidence to suggest that the influences of diameter and height aren't
similar for both tree types.

### 1 c)

We construct a linear model to investigate how diameter, height and type
influence volume.

```{r}
model = lm(volume~type+height+diameter, data=trees)
print("model coefficients:"); summary(model)$coefficients
print("anova:"); res = anova(model); res
```

<!--# TODO crispy code output^ -->

<!--# TODO: is the conclusion about type correct? -->

Based on the ANOVA p-values, type is not a significant predictor for
volume (p-value $0.143 > 0.05$), while height and diameter are
significant (p-values less than 0.05). Diameter and height are both
positively correlated with the volume, with diameter having the largest
contribution (coefficient) of the two.

```{r}
# build better model where type isn't considered
modelC = lm(volume~height+diameter, data=trees)

avgTree = data.frame(height=mean(trees$height), diameter=mean(trees$diameter))
pred = predict(modelC, avgTree)
sprintf("predicted volume of average tree = %.3f", pred)
# mean(trees$volume) # this also gives the same result as expected

r2 = summary(modelC)$r.squared; ar2 = summary(modelC)$adj.r.squared
sprintf("modelC: R^2 = %.3f, Adj. R^2 = %.3f", r2, ar2)
```

Using the resulting model, the volume of a tree with the average height
and diameter is predicted to be `r sprintf("%.3f", pred)` .

### 1 d)

We propose to transform the data to create a new column that contains
the volume of a (theoretical) cylinder based on the tree's diameter and
height. (Note we omit tree type from the model as we found it to not be
a significant predictor above).

<!--# TODO: ask what units are, because this volume calculation should consider them?? -->

```{r}
# create predictor as cylinderical volume
trees$cylinder = trees$diameter * pi * trees$height

modelD = lm(volume~cylinder, data=trees)
print("model coefficients:"); summary(modelD)$coefficients
r2 = summary(modelD)$r.squared; ar2 = summary(modelD)$adj.r.squared
sprintf("model: R^2 = %.3f, Adj. R^2 = %.3f", r2, ar2)
print("ANOVA:"); anova(model)
```

After constructing a linear model for predicting the actual tree volume
from our proposed cylindrical estimator, we see that the cylinder
variable is a significant predictor of volume (p \< 0.05). However the
adjusted $R^2$ values (and the regular $R^2$ values) for this model are
less than that of the model in part c), so while cylinder is a useful
predictor, it's still inferior to using just the provided height and
diameter variables in the model.

<!--# TODO: we could also argue that the cylinder field summarizes / reduces the dimensionality from R^2 -> R, so it's arguably a simpler model? -->

## Exercise 2: Expenditure on criminal activities

### 2 a)

```{r, fig.height=7}
crimes = read.table("expensescrime.txt", header=T)
pairs(crimes[,-1])
crimes$state = factor(crimes$state)


model = lm(expend~bad+crime+lawyers+employ+pop, data=crimes)
summary(model)$coefficients
anova(model)

print('model 2:')
model = lm(expend~crime+bad+lawyers+employ+pop, data=crimes)
summary(model)$coefficients
anova(model)

# sorting by population
#crimes[order(crimes$pop, decreasing=TRUE),]



#crimes
```

```{r, fig.height=3.5}
n = length(crimes[,1])
dists = cooks.distance(model)
plot(1:n, dists, type="b")
abline(1, 0, col = 'red') # plot y=1 for reference

# these are the indices into crimes that are cook's points
dists[dists > 1]
# TODO: print state names
#cooked = crimes[dists[dists > 1],]
#cooked

# investigating collinearity
cor(crimes[,-1])
res = cor(crimes[,-1])
# using 0.8 as a threshold to help with visiblility
res[res >= 0.8] = T; res[res <= 0.8] = F; 
res
```

Based on the correlation coefficients, it appears that all the
explanatory variables are correlated, except for crime which has no
correlation with any of the other variables (its highest correlation
coefficient is 0.375). The other variables all have a correlation
coefficient of at least 0.832 between each other.

<!--# TODO ask TA about what a fair threshold is? -->

### 2 b)

```{r}
evalModel = function(model, name) {
  print(sprintf("adding var '%s':", name))
  print(summary(model)$coefficients)
  r2 = summary(model)$r.squared; ar2 = summary(model)$adj.r.squared
  sprintf("model: R^2 = %.3f", r2)
}

evalModel(lm(expend~bad, data=crimes), name="bad")
evalModel(lm(expend~crime, data=crimes), name="crime")
evalModel(lm(expend~lawyers, data=crimes), name="lawyers")
evalModel(lm(expend~employ, data=crimes), name="employ")
evalModel(lm(expend~pop, data=crimes), name="pop")

# employ has highest adj. R^2 (0.955) and is significant
print("****round2****")
evalModel(lm(expend~employ+bad, data=crimes), name="bad")
evalModel(lm(expend~employ+crime, data=crimes), name="crime")
evalModel(lm(expend~employ+lawyers, data=crimes), name="lawyers")
evalModel(lm(expend~employ+pop, data=crimes), name="pop")
```

<!--# TODO: crispy code output -->

In the 1st round of the "step up" method we found "employ" to lead to
the largest $R^2$ model, while still being statistically significant.

In the 2nd round of the "step up" method, "lawyers" was found to lead to
the largest increase in $R^2$ while still being statistically
significant, however the increase in $R^2$ was only $0.963-0.954=0.009$,
which is quite low, so we don't deem it worth adding to the model.

The result of the "step up" method suggesting the model should only have
one explanatory variable ("employ") is not surprising as we showed
further above that all the variables (except for "crime") are collinear.

### 2 c)

<!--# Reference: Lecture 8, slide 13 -->

```{r}
model = lm(expend~employ, data=crimes) # result of part 2b
state = data.frame(bad=50, crime=5000, lawyers=5000, employ=5000, pop=5000)
predict(model, state, interval="prediction")
```

The predicted interval $[-407, 642]$ can be improved by adjusting it to
$[0, 642]$ as we know the expenditure must be a positive number. So
we're 95% confident that the expenditure by this hypothetical state
would be between \$0 and \$642,000.

### 2 d)

```{r}
x=as.matrix(crimes[,-1])
x=x[,-1]
y=crimes[,2]
x


train=sample(1:nrow(x),0.67*nrow(x))
x.train=x[train,]; y.train=y[train]
x.test=x[-train,]; y.test = y[-train] 

lm.model=lm(expend~bad+crime+lawyers+employ+pop,data=crimes,subset=train)
y.predict.lm=predict(lm.model,newdata=crimes[-train,])
mse.lm=mean((y.test-y.predict.lm)^2); mse.lm

mse.lm

library(glmnet) 
lasso.model=glmnet(x.train,y.train,alpha=1)
lasso.cv=cv.glmnet(x.train,y.train,alpha=1,type.measure="mse",nfolds=5)

plot(lasso.model,label=T,xvar="lambda")

plot(lasso.cv) 

lambda.min=lasso.cv$lambda.min; lambda.1se=lasso.cv$lambda.1se; 
lambda.min; lambda.1se 
coef(lasso.model,s=lasso.cv$lambda.min) 
coef(lasso.model,s=lasso.cv$lambda.1se) 

lasso.pred1=predict(lasso.model,s=lambda.min,newx=x.test) 
lasso.pred2=predict(lasso.model,s=lambda.1se,newx=as.matrix(x.test))
mse1.lasso=mean((y.test-lasso.pred1)^2); mse1.lasso
mse2.lasso=mean((y.test-lasso.pred2)^2); mse2.lasso
```
<!--# REMINDER: The output of the lasso model changes with every run, so make sure the conclusion fits with the output!!! -->

As we can see from the lambdas calculated by the model above, for the minimum error, the relevant variables are bad, crime, lawyers, and employ. To obtain a reduced model that is within one standard error of the minimum, we can take into account only bad, lawyers, and employ. In general the model is very similar to the one in b). While in b), the variables that were collinear could have been added to the model to improve the R^2, we chose not to do so because the improvement was marginal. The lasso model does something similar, in that it selects one of the aforementioned variables as the primary factor (which has a large coefficient), and the other (believed to be collinear) variables that are shown as relevant by the lasso model have very small coefficients, because as we saw, they had very little effect on the R^2 in b).

## Exercise 3: Titanic

### 3 a)

```{r}
titanic = read.table("titanic.txt", header=T)
#titanic
```

## Exercise 4: Military Coups

### 4 a)

```{r}
coups = read.table("coups.txt", header=T)
#coups
```
