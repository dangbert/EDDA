---
title: "Assignment 2: Group 45"
author: "Daniel Engbert, Rik Timmer, Koen van der Pool"
date: "03 March 2023"
output: pdf_document
fontsize: 11pt
highlight: tango
editor_options: 
  markdown: 
    wrap: 72
---

```{r, echo=FALSE}
options(digits=3) # e.g. for anova and summary() outputs...

# prevent code lines from being cutoff when too long:
#   (must install "formatR" package)
#   https://stackoverflow.com/a/66753995
#knitr::opts_chunk$set(tidy.opts = list(width.cutoff = 75), tidy = TRUE)

# helper function to check normality throughout this assignment
checkNorm = function(data, name, unit="") {
  xlab = name
  if (unit != "") {
    xlab = sprintf("%s (%s)", name, unit)
  }
  hist(data, main=sprintf("Histogram of %s", name), xlab=xlab)
  qqnorm(data, main=sprintf("Normal qqplot of %s", name))
  res = shapiro.test(data)
  print(sprintf("Shapiro-Wilk normality p-value for %s: %.3f", name, res$p.value))
  return(res$p.value)
}

checkAnovaNorm = function(model, name) {
  #qqnorm(residuals(model), main=sprintf("Normal Q-Q Plot: %s", name))
  plot(fitted(model), residuals(model))
  pval = checkNorm(residuals(model), sprintf("%s residuals", name))
  return(pval)
  #res = shapiro.test(residuals(model))
  #print(sprintf("Shapiro-Wilk normality p-value for %s residuals: %.3f", name, res$p.value))
}

printPval = function(pval) {
  print(sprintf("p-value = %.3f", pval))
}

conclude = function(pval, hypothesis, type="test") {
  if (pval < 0.05) {
    return(sprintf("the p-value $%.3f<0.05$ for this %s suggests is sufficient evidence to reject the $H_0$ (%s).", pval, type, hypothesis));
  } else {
    return(sprintf("the p-value $%.3f>0.05$ for this %s suggests there's insufficient evidence to reject the $H_0$ (%s).", pval, type, hypothesis));
  }
}
```

Note: we made a function `checkNorm()` which prints a histogram, qqplot,
and p-value from the shapiro-wilk normality test. And we made a function
`printPval()` which simply prints a given p-value to 3 significant
figures. We utilize both functions throughout this assignment.

## Exercise 1: Trees

### 1 a)

```{r, fig.height=2, fig.width=7, fig.show='hold'}
trees = read.table("treeVolume.txt", header=T)
trees$type = factor(trees$type)

par(mfrow=c(1,3))
model1 = lm(volume~type, data=trees)
pval = checkAnovaNorm(model1, "model1")

model2 = lm(log(volume)~type, data=trees)
pval = checkAnovaNorm(model2, "model2")

print("model coefficients:"); summary(model2)$coefficients
res = anova(model2)
sprintf("ANOVA p-value for type = %.3f", res["type", "Pr(>F)"])
```
We tested two models, and looked at the residuals of each for normality.  The results graphs suggests that using `log(volume)` in model2 is preferred as the resulting residuals are normally distributed (whereas this wasn't the case in model1).  Additionally, the ANOVA analysis of model2 suggests that we can reject the $H_0$ that `type` is not a significant predictor of (the log of) `volume` (as the p-value $0.035<0.05$).


```{r, fig.height=2.5, fig.width=6, echo=FALSE}
oaks = dplyr::filter(trees, type == "oak")
beeches = dplyr::filter(trees, type != "oak")
par(mfrow = c(1, 2))

pval = checkNorm(log(oaks$volume), "log(oak$volume)")
pval = checkNorm(log(beeches$volume), "log(beech$volume)")

sprintf("oak mean volume = %.3f, beech mean volume = %.3f", mean(oaks$volume), mean(beeches$volume))

# this is concistent with predictions in next part:
#sprintf("LOG: oak mean volume = %.3f, LOG beech mean volume = %.3f", #mean(log(oaks$volume)), mean(log(beeches$volume)))

# note:: we don't have to actually do this t test
#res = t.test(log(oaks$volume), log(beeches$volume))
#printPval(res$p.value)
```
We can split the data into two samples of tree volume based on the tree
types.  Checking normality, we see that the log transformed volumes of each tree type are normal (whereas the volumes themselves aren't as in 1a).

We can indeed compare the means of the transformed samples using a t-test to determine
whether, based on this data, there is a significant difference in mean
volume between the two tree types.

```{r}
new_oak = data.frame(type="oak"); new_beech = data.frame(type = "beech")
pred1 = exp(predict(model2, new_oak)); pred2 = exp(predict(model2, new_beech))
sprintf("predicted volumes: oak = %.3f, beech = %.3f", pred1, pred2)
```
Above we use `model2` to predict the log volume for each tree type, and use `exp()` to convert it to actual volume prediction.

### 1 b)

```{r, fig.height=2.5, fig.width=7}
par(mfrow = c(1, 3))
model = lm(log(volume)~type*diameter+height, data=trees)
pval = checkAnovaNorm(model, "log model")
res = drop1(model, test="F")
sprintf("drop1 p-value for type:diameter = %.3f", res["type:diameter", "Pr(>F)"])
```

We built a linear model that added an interaction term between diameter
and type,
`r conclude(res["type:diameter", "Pr(>F)"], "that the influence of diameter on volume is the same for both tree types", type="term")`

```{r, fig.height=2.5, fig.width=7}
model = lm(volume~type*height+diameter, data=trees)
res = drop1(model, test="F")
sprintf("drop1 p-value for type:diameter = %.3f", res["type:height", "Pr(>F)"])
```

Now running another linear model that includes an interaction term
between height and type instead,
`r conclude(res["type:height", "Pr(>F)"], "that the influence of height on volume is the same for both tree types", type="term")`

So based on the results from our two models above, there's insufficient
evidence to suggest that the influences of diameter and height aren't
similar for both tree types.

### 1 c)

We construct a linear model to investigate how diameter, height and type influence volume.

```{r, fig.height=2.5, fig.width=7, fig.show='hold'}
model = lm(volume~diameter+height+type, data=trees)
par(mfrow = c(1, 3)); pval = checkAnovaNorm(model, "model")
print("model coefficients:"); print(summary(model)$coefficients)
drop1(model, test="F")
```

We built a linear model (where the normality assumptions hold).  Based on the drop1 p-values, type is not a significant predictor for
volume (p-value $0.14 > 0.05$), while height and diameter are
significant (p-values less than 0.05). Diameter and height are both
positively correlated with the volume, with diameter having the largest
contribution (coefficient) of the two.

```{r, fig.height=2.5, fig.width=7}
# build better model where type isn't considered
modelC = lm(volume~height+diameter, data=trees)
par(mfrow = c(1, 3)); pval = checkAnovaNorm(modelC, "model")

avgTree = data.frame(height=mean(trees$height), diameter=mean(trees$diameter))
pred = predict(modelC, avgTree)
sprintf("predicted volume of average tree = %.3f", pred)
#mean(trees$volume) # this also gives the same result as expected

r2 = summary(modelC)$r.squared; ar2 = summary(modelC)$adj.r.squared
sprintf("modelC: R^2 = %.3f, Adj. R^2 = %.3f", r2, ar2)
```
We omitted type from our model as it was shown above to be insignificant, we also observe that the normality assumptions hold.  Using the resulting model, the volume of a tree with the average height and diameter is predicted to be `r sprintf("%.3f", pred)` .

### 1 d)

We propose to transform the data to create a new column that contains
the volume of a (theoretical) cylinder based on the tree's diameter and
height. (Note we omit tree type from the model as we found it to not be
a significant predictor above).

```{r}
# create predictor as cylinderical volume
trees$cylinder = pi * (trees$diameter / 2)^2 * trees$height
modelD = lm(volume~cylinder, data=trees)

print("model coefficients:"); summary(modelD)$coefficients
r2 = summary(modelD)$r.squared; ar2 = summary(modelD)$adj.r.squared
sprintf("model: R^2 = %.3f, Adj. R^2 = %.3f", r2, ar2)
```

After constructing a linear model for predicting the actual tree volume
from our proposed cylindrical estimator, we see that the cylinder
variable is a significant predictor of volume (p \< 0.05). The adjusted
$R^2$ values (and the regular $R^2$ values) for this model are both
greater than that of the model in part c), so this model appears to be
superior to using just the provided height and diameter variables in the
model.

## Exercise 2: Expenditure on criminal activities

### 2 a)

```{r, fig.height=4.5}
crimes = read.table("expensescrime.txt", header=T)
pairs(crimes[,-1])
crimes$state = factor(crimes$state)
model = lm(expend~crime+bad+lawyers+employ+pop, data=crimes)
summary(model)$coefficients; anova(model)
```

```{r, fig.height=2.7, fig.width=5}
n = length(crimes[,1])
dists = cooks.distance(model)
plot(1:n, dists, type="b", main="Cook's Distance by Dataset Index")
abline(1, 0, col = 'red') # plot y=1 for reference

# cook's points:
print("Influence points:"); crimes[dists > 1,]

# investigating collinearity:
cor(crimes[,-1])
# using 0.8 as a threshold to help with visiblility:
res = cor(crimes[,-1])
res[res >= 0.8] = T; res[res <= 0.8] = F; res
```

Based on the correlation coefficients, it appears that all the
explanatory variables are correlated with each other, except for crime
which has no correlation with any of the other variables (its highest
correlation coefficient is 0.375). The other variables all have a
correlation coefficient of at least 0.832 between each other.

### 2 b)

```{r}
evalModel = function(model, name) {
  r2 = summary(model)$r.squared; ar2 = summary(model)$adj.r.squared
  pVal = summary(model)$coefficients[name, "Pr(>|t|)"]
  cat(sprintf("trying var '%s'\t\tPr(>|t|) =  %.3f, model R^2 = %.3f\n", name, pVal, r2))
}

doStepUp = function() {
  cat("\n****round1 (building model with 1 var)****\n")
  evalModel(lm(expend~bad, data=crimes), name="bad")
  evalModel(lm(expend~crime, data=crimes), name="crime")
  evalModel(lm(expend~lawyers, data=crimes), name="lawyers")
  evalModel(lm(expend~employ, data=crimes), name="employ")
  evalModel(lm(expend~pop, data=crimes), name="pop")
  
  # employ has highest adj. R^2 (0.954) and is significant
  cat("\n****round2 (building on model with employ)****\n")
  evalModel(lm(expend~employ+bad, data=crimes), name="bad")
  evalModel(lm(expend~employ+crime, data=crimes), name="crime")
  evalModel(lm(expend~employ+lawyers, data=crimes), name="lawyers")
  evalModel(lm(expend~employ+pop, data=crimes), name="pop")
  
  cat("\n****round3 (building on model with employ+laywers)****\n")
  evalModel(lm(expend~employ+lawyers+bad, data=crimes), name="bad")
  evalModel(lm(expend~employ+lawyers+crime, data=crimes), name="crime")
  evalModel(lm(expend~employ+lawyers+pop, data=crimes), name="pop")
}
doStepUp()
```

In the 1st round of the "step up" method we found "employ" to lead to
the model with the largest $R^2$, while still being statistically
significant. In the 2nd round (building up from the model with "employ"), "lawyers" was found to lead to the
largest increase in $R^2$ while still being statistically significant (p < 0.05), so we add it to the model.  The 3rd round of the "step up" method showed that no further variables could be added while being statistically significant.  So our final model is `lm(expend~employ+lawyers, data=crimes)`.

### 2 c)
<!--# Reference: Lecture 8, slide 13 -->
Taking the resulting model from part 2b, we found the normality assumptions weren't met, so we adjusted the model by taking the square root of `expend`.  The residuals then indicated normality assumptions were met, however the p-value for `lawyers` was no longer significant.

So we tried the best model from round 1 of step up which was `lm(expend~employ, data=crimes)` (noting that this model was comporable as adding `laywers` had only increased the $R^2$ by $0.963-0.954=0.009$).  Finally, after checking the normality assumptions of this model, we again saw they weren't met, and so settled on the final model of `lm(sqrt(expend)~employ, data=crimes)` for which the assumptions are now met (the qqplot of residuals below is approximately linear now).

```{r, fig.height=2.5, fig.width=6}
# this model meets normality assumptions, but p-value for lawyers is 0.44 > 0.05
#model = lm(sqrt(expend)~employ+lawyers, data=crimes)

model = lm(sqrt(expend)~employ, data=crimes)
summary(model)$coefficients

par(mfrow = c(1, 3)); pval = checkAnovaNorm(model, "model")

state = data.frame(bad=50, crime=5000, lawyers=5000, employ=5000, pop=5000)
pred = predict(model, state, interval="prediction")^2

sprintf("predicted expenditure CI ="); pred
```

The predicted interval $[80.3, 526]$ can't be further improved in this case, as both numbers are $>0$ and the process of finding a solid model already improved the accuracy of this CI.  Arguably, you could consider removing some influence points to further "improve" this CI (but this is risky).

### 2 d)

```{r, results='hide', message=F}
library(glmnet)
```

```{r, fig.height=2.5, fig.width=5.5}
par(mfrow=c(1,2))
set.seed(42) # ensuring results don't change each time its run
x = as.matrix(crimes[,-1]) # remove states column
x = x[,-1] # remove expenditure
y = crimes[,2]

train = sample(1:nrow(x),0.67*nrow(x))
x.train = x[train,]; y.train = y[train]
x.test = x[-train,]; y.test = y[-train] 

lm.model = lm(expend~bad+crime+lawyers+employ+pop,data=crimes, subset=train)
y.predict.lm = predict(lm.model,newdata=crimes[-train,])
mse.lm = mean((y.test-y.predict.lm)^2)
sprintf("linear model mse = %.3f", mse.lm)

lasso.model = glmnet(x.train,y.train,alpha=1)
lasso.cv = cv.glmnet(x.train,y.train,alpha=1,type.measure="mse",nfolds=5)

plot(lasso.model,label=T,xvar="lambda"); plot(lasso.cv) 

lambda.min = lasso.cv$lambda.min; lambda.1se=lasso.cv$lambda.1se; 
sprintf("lambda.min = %.3f, lambda.1se = %.3f", lambda.min, lambda.1se)

# lasso min (minimizing R^2)
coef(lasso.model,s=lasso.cv$lambda.min)
# 1 SE (minimizing number of vars, while remaining within 1 std. err.)
coef(lasso.model,s=lasso.cv$lambda.1se) 

lasso.pred1 = predict(lasso.model,s=lambda.min,newx=x.test) 
lasso.pred2 = predict(lasso.model,s=lambda.1se,newx=as.matrix(x.test))
mse1.lasso = mean((y.test-lasso.pred1)^2)
mse2.lasso = mean((y.test-lasso.pred2)^2)
sprintf("mse1 = %.3f, mse2 = %.3f", mse1.lasso, mse2.lasso)
```

<!--# REMINDER: The output of the lasso model changes with every run, so make sure the conclusion fits with the output!!! -->

As we can see from the lambdas calculated by the model above, for the
minimum error, the relevant variables are `laywers`, `employ`, and `crime`.  To obtain a reduced model that is within one standard error of the minimum, we can take into account only these relevant variables.

Considering the scale differences of the relevant variables, the coefficient of `pop` is not only the largest, but it also has the largest contribution for predictions (as the `pop` variable will always be larger than `employ` and `lawyer` for any given state by a considerable margin).

As a disclaimer, the results from the lasso method will always vary with random chance, however this intrerpretation holds for our resulting model.

The coefficients of the other two relevant variables (`lawyers` and `pop`) are much smaller due to their collinearity with `employ`.

## Exercise 3: Titanic

### 3 a)

```{r, fig.height=3.5, tidy=TRUE, tidy.opts=list(width.cutoff=60)}
titanic = read.table("titanic.txt", header=T)
titanic$PClass = factor(titanic$PClass)
titanic$Sex = factor(titanic$Sex)
#titanic$Survived = factor(titanic$Survived)

library(plyr)
# round age to nearest 10 years (https://stackoverflow.com/a/6466894)
titanic$Ager = round_any(titanic$Age, 20)
titanic$Ager = factor(titanic$Ager)
#titanic$Age2 = titanic$Age^2

print("total number of individuals for each combination of class and gender:")
tot = xtabs(~PClass+Sex, data=titanic); tot

print("Survival rate by class and gender combinations:")
totc = xtabs(Survived~PClass+Sex, data=titanic); round(totc/tot, 2)

#plot(titanic$Age, y=titanic$Survived, xlim=c(0, 100))
totage = xtabs(~Age, data=titanic)
barplot(xtabs(Survived~Age,data=titanic)/totage, main="% Survival by Age Group", xlab="Age Group", ylab="Survival Ratio")

# build logistic regression model
model = glm(Survived~Age+Sex+PClass,data=titanic, family=binomial)
drop1(model, test = "Chisq")
summary(model)$coefficients

plot(titanic$Age,exp(3.759662 + titanic$Age*-0.039177), ylab="Odds of Survival", xlab="Age (years)",main="Survival Odds of a First Class Female by Age")
```

<!--# TODO: Look at going for age vs ager when attempting to interpret odds -->

<!--# TODO: larger binsize for boxplot... -->

In the model summary above, the intercept can be interpreted as a female who traveled in 1st class. We can see that other classes (namely 2nd and
3rd) lower the odds of survival by the negative coefficients, where 3rd
class has a larger impact than 2nd. Similarly, an increase in age has a
negative impact on survival odds, however the impact is low because of
the parabolic shape of the graph shown above. How we interpret this is
that the fact that very young and very old people have the highest
survival rates are balancing each other out to form a coefficient that
is close to 0, which inaccurately represents the actual relationship
between age and survival, because it is clear that it does have an
effect. Finally the odds of survival of a male are also significantly
lower than that of a female, as shown by the negative coefficient.

### 3 b)

```{r, fig.height=2.75, fig.width=5}
ageclass_model = glm(Survived~Age*PClass+Sex,data=titanic, family=binomial)
summary(ageclass_model)$coefficients

agesex_model = glm(Survived~Age*Sex+PClass,data=titanic, family=binomial)
summary(agesex_model)$coefficients

male1 = data.frame(Age=55, Sex = "male", PClass = "1st")
male2 = data.frame(Age=55, Sex = "male", PClass = "2nd")
male3 = data.frame(Age=55, Sex = "male", PClass = "3rd")
female1 = data.frame(Age=55, Sex = "female", PClass = "1st")
female2 = data.frame(Age=55, Sex = "female", PClass = "2nd")
female3 = data.frame(Age=55, Sex = "female", PClass = "3rd")

predict(model,female1, type="response")
predict(model,female2, type="response")
predict(model,female3, type="response")
predict(model,male1, type="response")
predict(model,male2, type="response")
predict(model,male3, type="response")
```

Given the results of the two models produced here, as well as the one
from 3a), we will select the model that considers the interaction
between sex and age, given that the model found that the interaction was
significant (the model in 3a) found that sex and age by themselves were
also significant). Furthermore, the model that considered the
interaction between age and class found that the interaction was not
significant ($P > 0.05$ for both 2nd class and 3rd class).

The estimates for the probability of survival of each of the
combinations of factors is as shown below:

<!--# TODO: is it okay to round ages? -->

<!--# TODO: clean up after me pls daddy Dan -->

### 3 c)

We propose to do k-fold cross-validation, using 10% of the data in each
fold, and calculating the average prediction accuracy per combination of
factor levels. We would use a slightly modified version of the model
from 3b), namely
`glm(Survived~Ager*Sex+PClass,data=titanic, family=binomial)` , where
the difference would be that the age would be split into several groups
(represented as the variable `Ager`.

To be able to predict for the odds resulting from the model, we would
use a threshold of 1, where if the odds were above 1, the subject would
be predicted as survived and not survived otherwise. (Because when the
odds are $\ge 1$ , there's a $\ge 50\%$ chance of survival).

### 3 d)

Based on the (3x2) table in part 3a, every unique combination of classes
in this dataset has $>5$ individuals, so a chi-squared test is
applicable.

To investigate the effect of the factors `PClass` and `Sex` separately,
we'll perform two separate chisquare tests which each test the
significance of a single factor.

```{r}
# build contigency table
cont1 = as.matrix(xtabs(~PClass, data=titanic)); cont1
chisq.test(cont1)

cont2 = as.matrix(xtabs(~Sex, data=titanic)); cont2
chisq.test(cont2)
```

Both chisquare tests yield p-values $<0.05$ so we reject the null
hypotheses that Sex and PClass aren't significantly explanatory
variables for predicting survival.

### 3 e)

Comparing the linear regression approach vs the chisquare approach, the
linear regression approach has the advantage of being able to make
predictions on a given individual's chance of survival (which chisquare
has no ability to do), and the linear regression approach can give
p-values for each variable.

Both methods can also give insights into the significance of explanatory
variables.

## Exercise 4: Military Coups

### 4 a)

```{r}
coups = read.table("coups.txt", header=T)
coups$pollib=factor(coups$pollib)

coupsglm = glm(miltcoup~oligarchy+pollib+parties+pctvote+popn+size+numelec+numregim, family=poisson, data=coups)
summary(coupsglm)$coefficients
```

As can be seen by the results of the Poisson model above, it is seen
that the variables $oligarchy$, $pollib$, and $parties$ are seen as
significant in predicting the number of successful military coups from
independence to 1989.

### 4 b)

```{r,tidy=TRUE, tidy.opts=list(width.cutoff=60)}
VERBOSE = F
# prints just the top 3 least significant coefficients of a model:
evalModel = function(model, name, verbose=VERBOSE) {
  if (verbose) {
    cat(sprintf("\n%s:\n", name))
    res = drop1(model, test="Chisq")
    #res = summary(model)$coefficients
    print(head(res[order(res[,5], decreasing=T),], n=3))
  }
}

coupsglm1 = glm(miltcoup~oligarchy+pollib+parties+pctvote+popn+size+numelec+numregim, family=poisson, data=coups)
evalModel(coupsglm1, "coupsglm1")
# numelec has highest p-value (0.67) so we remove it

coupsglm2 = glm(miltcoup~oligarchy+pollib+parties+pctvote+popn+size+numregim, family=poisson, data=coups)
evalModel(coupsglm2, "coupsglm2")
# numregim has highest p-value (0.42) so we remove it

coupsglm3 = glm(miltcoup~oligarchy+pollib+parties+pctvote+popn+size, family=poisson, data=coups)
evalModel(coupsglm3, "coupsglm3")
# size has highest p-value (0.33) so we remove it

coupsglm4 = glm(miltcoup~oligarchy+pollib+parties+pctvote+popn, family=poisson, data=coups)
evalModel(coupsglm4, "coupsglm4")
# popn has highest p-value (0.31) so we remove it

coupsglm5 = glm(miltcoup~oligarchy+pollib+parties+pctvote, family=poisson, data=coups)
evalModel(coupsglm5, "coupsglm5")
# pctvote has highest p-value (0.19) so we remove it

coupsglm6 = glm(miltcoup~oligarchy+pollib+parties, family=poisson, data=coups)
evalModel(coupsglm6, "coupsglm6")
print("coupsglm6 (final model):")
drop1(coupsglm6, test="Chisq")
```


The number of explanatory variables was reduced following the step-down
approach, and in doing so the variables were removed in the following
order: $numelec$, $numregim$, $size$ $popn$, $pctvote$. This left the
model (`coupsglm6`) with only relevant explanatory variables (significant as
$P < 0.05$). These variables were the same as those seen as relevant in
the previous model in 4a), namely $oligarchy$, $pollib$, and $parties$.

### 4 c)

```{r}
country1 = data.frame(oligarchy = mean(coups$oligarchy),
  pollib = factor(0), parties = mean(coups$parties))
country2 = data.frame(oligarchy = mean(coups$oligarchy),
  pollib = factor(1), parties = mean(coups$parties))
country3 = data.frame(oligarchy = mean(coups$oligarchy),
  pollib = factor(2), parties = mean(coups$parties))

printResults = function() {
  print(sprintf("country1 prediction = %.3f", predict(coupsglm6, country1, type="response")))
  print(sprintf("country2 prediction = %.3f", predict(coupsglm6, country2, type="response")))
  print(sprintf("country3 prediction = %.3f", predict(coupsglm6, country3, type="response")))
}
printResults()
```

Using the model from 4b), we found that the coefficient of $pollib$ was
significant and negative. This entails that with an increase in the
value of the political liberalization (i.e. with an increase in civil
rights), there would be a decrease in the predicted number of successful
military coups. The predictions above confirm this.
